{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SzHmDB9RnoLi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import mnist\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz-njHpjoJff",
        "outputId": "8bce4688-852f-456c-8b0f-53b666e209dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train) , (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") /255.0\n",
        "x_test - x_test.astype(\"float32\") /255.0\n",
        "\n",
        "model = keras.Sequential()\n",
        "# why we are specifying None we do not have to have a specific time step\n",
        "model.add(keras.Input(shape=(None, 28)))\n",
        "model.add(\n",
        "    # The return_sequence is that it is returning each output from a time step, we can stack multiple time stack\n",
        "    layers.SimpleRNN(256, return_sequences=True)\n",
        ")\n",
        "model.add(layers.SimpleRNN(256, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = keras.optimizers.Adam(lr=0.01),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)\n",
        "\n",
        "# print(model.summary())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Runing a GRU Code\n",
        "(x_train, y_train) , (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") /255.0\n",
        "x_test - x_test.astype(\"float32\") /255.0\n",
        "\n",
        "model = keras.Sequential()\n",
        "# why we are specifying None we do not have to have a specific time step\n",
        "model.add(keras.Input(shape=(None, 28)))\n",
        "model.add(\n",
        "    # The return_sequence is that it is returning each output from a time step, we can stack multiple time stack\n",
        "    layers.GRU(256, return_sequences=True)\n",
        ")\n",
        "model.add(layers.GRU(256, activation='relu'))\n",
        "\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = keras.optimizers.Adam(lr=0.01),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Runing a LSM Code\n",
        "(x_train, y_train) , (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") /255.0\n",
        "x_test - x_test.astype(\"float32\") /255.0\n",
        "\n",
        "model = keras.Sequential()\n",
        "# why we are specifying None we do not have to have a specific time step\n",
        "model.add(keras.Input(shape=(None, 28)))\n",
        "model.add(\n",
        "    # The return_sequence is that it is returning each output from a time step, we can stack multiple time stack\n",
        "    layers.Bidirectional(\n",
        "        layers.LSTM(256, return_sequences=True, activation='tanh')\n",
        "        )\n",
        ")\n",
        "model.add(\n",
        "    layers.Bidirectional(\n",
        "         layers.LSTM(256, activation='tanh')\n",
        "    )\n",
        "   \n",
        ")\n",
        "\n",
        "model.add(layers.Dense(10))\n",
        "\n",
        "model.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = keras.optimizers.Adam(lr=0.01),\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)\n",
        "\n",
        "print(model.summary())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDviS36bzICr"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "RNN_GRU_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwxMyWZD7TtpR1nxd1yQfB"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}